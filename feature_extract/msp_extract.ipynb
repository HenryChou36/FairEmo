{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 22:19:36 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "import fairseq\n",
    "import librosa\n",
    "import torch\n",
    "import speechbrain as sb\n",
    "import torch\n",
    "import os\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from tqdm import tqdm\n",
    "import hdbscan\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_distances, pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialized Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('/mnt/sda/hsinghang/dataset/MSP-Podcast-v1.10/labels/labels_consensus.csv')\n",
    "df = pd.DataFrame(columns=['file_name', 'emotion', 'A', 'V', 'D', 'spk_id', 'gender', 'split', 'wav_path', 'hubert_path', 'hubert_len', 'emb_xvec', 'hdb_label'])\n",
    "df['file_name'] = labels['FileName']\n",
    "df['emotion'] = labels['EmoClass']\n",
    "df['A'] = labels['EmoAct']\n",
    "df['V'] = labels['EmoVal']\n",
    "df['D'] = labels['EmoDom']\n",
    "df['spk_id'] = labels['SpkrID']\n",
    "df['gender'] = labels['Gender']\n",
    "df['split'] = labels['Split_Set']\n",
    "\n",
    "# modify this to make it point to where your wav files are stored\n",
    "df['wav_path'] = '/mnt/sda/hsinghang/dataset/MSP-Podcast-v1.10/audios/' + df['file_name']\n",
    "# modify following to make them points to where you want to store these features\n",
    "df['hubert_path'] = '/mnt/sda/hsinghang/dataset/MSP-Podcast-v1.10/features/hubert/' + df['file_name'].str.replace('.wav', '.pkl')\n",
    "df['emb_xvec'] = '/mnt/sda/hsinghang/dataset/MSP-Podcast-v1.10/features/emb/xvec/' + df['file_name'].str.replace('.wav', '.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Hubert Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# check point path\n",
    "ckpt_path = '/homes/hsinghang/model/hubert/hubert_base_ls960.pt'\n",
    "models, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([ckpt_path])\n",
    "model = models[0]\n",
    "model.to(device)\n",
    "\n",
    "wav_list = list(df['wav_path'])\n",
    "out_list = list(df['hubert_path'])\n",
    "seq_len = [0] * len(out_list)\n",
    "for c, wav_path in enumerate(tqdm(wav_list)):\n",
    "    #s, sr = sf.read(wav_path)\n",
    "    #print(sr)\n",
    "    s, sr = librosa.load(wav_path, sr=16000)\n",
    "    assert s.ndim == 1, s.ndim\n",
    "    feats_audio = torch.FloatTensor(s).reshape((1, -1))\n",
    "    with torch.no_grad():\n",
    "        feats_audio = feats_audio.to(device)\n",
    "        z = model.extract_features(feats_audio)[0]\n",
    "        z = z.cpu().detach().numpy().squeeze()\n",
    "    seq_len[c] = len(z)\n",
    "    out_path = '/'.join(out_list[c].split('/')[:-1])\n",
    "    path = Path(out_path)\n",
    "    if not os.path.exists(str(path.as_posix())):\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    joblib.dump(z, out_list[c])\n",
    "    if c % 1000 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "#iemocap_meta['hubert_feature'] = out_list\n",
    "df['hubert_len'] = seq_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Speaker Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "run_opts={\"device\":\"cuda\"} \n",
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    wav_path = row['wav_path']\n",
    "    emb_path = row['emb_xvec']\n",
    "    signal, fs = torchaudio.load(wav_path)\n",
    "    #signal = signal.to(device)\n",
    "    embs = classifier.encode_batch(signal)\n",
    "    joblib.dump(embs.cpu().numpy().squeeze(), emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Development', 'Test1', 'Test2', 'Train'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(df['split'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['split'] == 'Train']\n",
    "valid_df = df[df['split'] == 'Development']\n",
    "test_df = df[(df['split'] == 'Test1') | (df['split'] == 'Test2')]\n",
    "# I think we actually only use test 1 to evaluate fairness since Test 2 are speakers with unknown id and is filtered out by data_loader.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>emotion</th>\n",
       "      <th>A</th>\n",
       "      <th>V</th>\n",
       "      <th>D</th>\n",
       "      <th>spk_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>split</th>\n",
       "      <th>wav_path</th>\n",
       "      <th>hubert_path</th>\n",
       "      <th>hubert_len</th>\n",
       "      <th>emb_xvec</th>\n",
       "      <th>hdb_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [file_name, emotion, A, V, D, spk_id, gender, split, wav_path, hubert_path, hubert_len, emb_xvec, hdb_label]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cluster label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
