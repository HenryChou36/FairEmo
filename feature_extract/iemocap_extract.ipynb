{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import fairseq\n",
    "import librosa\n",
    "import torch\n",
    "import torch\n",
    "import os\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "import torchaudio\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import hdbscan\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialized Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('/mnt/sda/hsinghang/dataset/IEMOCAP/IEMOCAP_Org_Emo.csv')\n",
    "df = pd.DataFrame(columns=['file_name', 'emotion', 'A', 'V', 'D', 'spk_id', 'gender', 'split', 'wav_path', 'hubert_path', 'hubert_len', 'emb_xvec', 'hdb_label'])\n",
    "df['file_name'] = labels['Name_String'].str.replace('.txt', '.wav')\n",
    "df['emotion'] = labels['Emotion']\n",
    "df['A'] = labels['Activation']\n",
    "df['V'] = labels['Valence']\n",
    "df['D'] = labels['Dominance']\n",
    "df['spk_id'] = labels['Name_String'].str[3:5].astype('int') + (labels['Name_String'].str[5] == 'F') * 5\n",
    "df['gender'] = np.where(labels['Name_String'].str[5] == 'F', 'Female', 'Male')\n",
    "\n",
    "# modify this to make it point to where your wav files are stored\n",
    "df['wav_path'] = '/mnt/sda/hsinghang/dataset/IEMOCAP/audios/' + df['file_name']\n",
    "# modify following to make them points to where you want to store these features\n",
    "df['hubert_path'] = '/mnt/sda/hsinghang/dataset/IEMOCAP/features/hubert/' + df['file_name'].str.replace('.wav', '.pkl')\n",
    "df['emb_xvec'] = '/mnt/sda/hsinghang/dataset/IEMOCAP/features/emb/xvec/' + df['file_name'].str.replace('.wav', '.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Hubert Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# check point path\n",
    "ckpt_path = '/homes/hsinghang/model/hubert/hubert_base_ls960.pt'\n",
    "models, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([ckpt_path])\n",
    "model = models[0]\n",
    "model.to(device)\n",
    "\n",
    "wav_list = list(df['wav_path'])\n",
    "out_list = list(df['hubert_path'])\n",
    "seq_len = [0] * len(out_list)\n",
    "for c, wav_path in enumerate(tqdm(wav_list)):\n",
    "    s, sr = librosa.load(wav_path, sr=16000)\n",
    "    assert s.ndim == 1, s.ndim\n",
    "    feats_audio = torch.FloatTensor(s).reshape((1, -1))\n",
    "    with torch.no_grad():\n",
    "        feats_audio = feats_audio.to(device)\n",
    "        z = model.extract_features(feats_audio)[0]\n",
    "        z = z.cpu().detach().numpy().squeeze()\n",
    "    seq_len[c] = len(z)\n",
    "    out_path = '/'.join(out_list[c].split('/')[:-1])\n",
    "    path = Path(out_path)\n",
    "    if not os.path.exists(str(path.as_posix())):\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    joblib.dump(z, out_list[c])\n",
    "    if c % 1000 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "df['hubert_len'] = seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Speaker Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "run_opts={\"device\":\"cuda\"} \n",
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    wav_path = row['wav_path']\n",
    "    emb_path = row['emb_xvec']\n",
    "    signal, fs = torchaudio.load(wav_path)\n",
    "    #signal = signal.to(device)\n",
    "    embs = classifier.encode_batch(signal)\n",
    "    joblib.dump(embs.cpu().numpy().squeeze(), emb_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = 0.7 \n",
    "valid_portion = 0.1\n",
    "test_portion = 0.2\n",
    "\n",
    "np.random.seed(seed=819)\n",
    "train_split = []\n",
    "valid_split = []\n",
    "test_split = []\n",
    "for id in np.unique(df['spk_id']):\n",
    "    spk_indices = (df[df['spk_id'] == id]).index.to_numpy()\n",
    "    np.random.shuffle(spk_indices)\n",
    "    data_len = len(spk_indices)\n",
    "    train_split.append(spk_indices[:int(data_len * train_portion)])\n",
    "    valid_split.append(spk_indices[int(data_len * train_portion): int(data_len * (train_portion + valid_portion))])\n",
    "    test_split.append(spk_indices[int(data_len * (train_portion + valid_portion)):])\n",
    "\n",
    "train_split = np.sort(np.concatenate(train_split))\n",
    "valid_split = np.sort(np.concatenate(valid_split))\n",
    "test_split = np.sort(np.concatenate(test_split))\n",
    "train_df = df.loc[train_split].copy()\n",
    "valid_df = df.loc[valid_split].copy()\n",
    "test_df = df.loc[test_split].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create cluster label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get speaker embedding for each set\n",
    "MIN_SAMPLE = 32\n",
    "train_emb = []\n",
    "for emb in tqdm(train_df['emb_xvec']):\n",
    "    train_emb.append(joblib.load(emb))\n",
    "train_emb = np.array(train_emb)\n",
    "\n",
    "valid_emb = []\n",
    "for emb in tqdm(valid_df['emb_xvec']):\n",
    "    valid_emb.append(joblib.load(emb))\n",
    "valid_emb = np.array(valid_emb)\n",
    "\n",
    "test_emb = []\n",
    "for emb in tqdm(test_df['emb_xvec']):\n",
    "    test_emb.append(joblib.load(emb))\n",
    "test_emb = np.array(test_emb)\n",
    "\n",
    "train_v, train_c = np.unique(train_df['spk_id'], return_counts=True)\n",
    "train_v = train_v[train_c >= MIN_SAMPLE]\n",
    "train_v = train_v[train_v != 'Unknown']\n",
    "selected_train_emb = train_emb[np.isin(train_df['spk_id'].to_numpy(), train_v)]\n",
    "\n",
    "valid_v, c = np.unique(valid_df['spk_id'], return_counts=True)\n",
    "valid_v = valid_v[c >= MIN_SAMPLE]\n",
    "valid_v = valid_v[valid_v != 'Unknown']\n",
    "selected_valid_emb = valid_emb[np.isin(valid_df['spk_id'].to_numpy(), valid_v)]\n",
    "\n",
    "test_v, c = np.unique(test_df['spk_id'], return_counts=True)\n",
    "test_v = test_v[c >= MIN_SAMPLE]\n",
    "test_v = test_v[test_v != 'Unknown']\n",
    "selected_test_emb = test_emb[np.isin(test_df['spk_id'].to_numpy(), test_v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform PCA for dimension reduction\n",
    "pca = PCA(n_components=.80)\n",
    "pca.fit(selected_train_emb)\n",
    "selected_pca_train_emb = pca.transform(selected_train_emb)\n",
    "selected_pca_valid_emb = pca.transform(selected_valid_emb)\n",
    "selected_pca_test_emb = pca.transform(selected_test_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clusterer = hdbscan.HDBSCAN(min_cluster_size=32, min_samples=4)\n",
    "train_clusterer.fit(selected_pca_train_emb)\n",
    "\n",
    "valid_clusterer = hdbscan.HDBSCAN(min_cluster_size=32, min_samples=4)\n",
    "valid_clusterer.fit(selected_pca_valid_emb)\n",
    "\n",
    "test_clusterer = hdbscan.HDBSCAN(min_cluster_size=32, min_samples=4)\n",
    "test_clusterer.fit(selected_pca_test_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit noise node\n",
    "train_cluster_avg_emb = []\n",
    "for i in range(train_clusterer.labels_.max()):\n",
    "    train_cluster_avg_emb.append(np.mean(selected_pca_train_emb[train_clusterer.labels_ == i], axis=0, keepdims=False))\n",
    "train_cluster_avg_emb = np.array(train_cluster_avg_emb)\n",
    "\n",
    "valid_cluster_avg_emb = []\n",
    "for i in range(valid_clusterer.labels_.max()):\n",
    "    valid_cluster_avg_emb.append(np.mean(selected_pca_valid_emb[valid_clusterer.labels_ == i], axis=0, keepdims=False))\n",
    "valid_cluster_avg_emb = np.array(valid_cluster_avg_emb)\n",
    "\n",
    "test_cluster_avg_emb = []\n",
    "for i in range(test_clusterer.labels_.max()):\n",
    "    test_cluster_avg_emb.append(np.mean(selected_pca_test_emb[test_clusterer.labels_ == i], axis=0, keepdims=False))\n",
    "test_cluster_avg_emb = np.array(test_cluster_avg_emb)\n",
    "\n",
    "train_noise_similarity = cosine_similarity(selected_pca_train_emb[train_clusterer.labels_==-1], train_cluster_avg_emb)\n",
    "valid_noise_similarity = cosine_similarity(selected_pca_valid_emb[valid_clusterer.labels_==-1], valid_cluster_avg_emb)\n",
    "test_noise_similarity = cosine_similarity(selected_pca_test_emb[test_clusterer.labels_==-1], test_cluster_avg_emb)\n",
    "\n",
    "train_noise_fitted_label = np.copy(train_clusterer.labels_)\n",
    "j = 0\n",
    "for i, clust_lab in enumerate(train_clusterer.labels_):\n",
    "    if clust_lab == -1:\n",
    "        train_noise_fitted_label[i] = np.argmax(train_noise_similarity[j])\n",
    "        j += 1\n",
    "\n",
    "valid_noise_fitted_label = np.copy(valid_clusterer.labels_)\n",
    "j = 0\n",
    "for i, clust_lab in enumerate(valid_clusterer.labels_):\n",
    "    if clust_lab == -1:\n",
    "        valid_noise_fitted_label[i] = np.argmax(valid_noise_similarity[j])\n",
    "        j += 1\n",
    "test_noise_fitted_label = np.copy(test_clusterer.labels_)\n",
    "j = 0\n",
    "for i, clust_lab in enumerate(test_clusterer.labels_):\n",
    "    if clust_lab == -1:\n",
    "        test_noise_fitted_label[i] = np.argmax(test_noise_similarity[j])\n",
    "        j +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make labels out of expieriment to -2\n",
    "hdb_labels = np.ones(len(train_df['file_name'])) * -2\n",
    "hdb_labels[np.isin(train_df['spk_id'].to_numpy(), train_v)] = train_noise_fitted_label\n",
    "train_df['hdb_label'] = hdb_labels.astype('int')\n",
    "\n",
    "hdb_labels = np.ones(len(valid_df['file_name'])) * -2\n",
    "hdb_labels[np.isin(valid_df['spk_id'].to_numpy(), valid_v)] = valid_noise_fitted_label\n",
    "valid_df['hdb_label'] = hdb_labels.astype('int')\n",
    "\n",
    "hdb_labels = np.ones(len(test_df['file_name'])) * -2\n",
    "hdb_labels[np.isin(test_df['spk_id'].to_numpy(), test_v)] = test_noise_fitted_label\n",
    "test_df['hdb_label'] = hdb_labels.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save result meta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./feature_extract/IEMOCAP/train_meta_hdb.csv', index=True)\n",
    "valid_df.to_csv('./feature_extract/IEMOCAP/valid_meta_hdb.csv', index=True)\n",
    "test_df.to_csv('./feature_extract/IEMOCAP/test_meta_hdb.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
